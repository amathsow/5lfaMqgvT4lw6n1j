{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Background:**\n",
    "\n",
    "Our company develops innovative Artificial Intelligence and Computer Vision solutions that revolutionize industries. Machines that can see: We pack our solutions in small yet intelligent devices that can be easily integrated to your existing data flow. Computer vision for everyone: Our devices can recognize faces, estimate age and gender, classify clothing types and colors, identify everyday objects and detect motion. Technical consultancy: We help you identify use cases of artificial intelligence and computer vision in your industry. Artificial intelligence is the technology of today, not the future.\n",
    "\n",
    "MonReader is a new mobile document digitization experience for the blind, for researchers and for everyone else in need for fully automatic, highly fast and high-quality document scanning in bulk. It is composed of a mobile app and all the user needs to do is flip pages and everything is handled by MonReader: it detects page flips from low-resolution camera preview and takes a high-resolution picture of the document, recognizing its corners and crops it accordingly, and it dewarps the cropped document to obtain a bird's eye view, sharpens the contrast between the text and the background and finally recognizes the text with formatting kept intact, being further corrected by MonReader's ML powered redactor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"mobile.png\" width=400 height=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MonReader is a new mobile document digitalization experience for the blind, for researchers and for everyone else in need for fully automatic, highly fast and high-quality document scanning in bulk. It is composed of a mobile app and all the user needs to do is flip pages and everything is handled by MonReader: it detects page flips from low-resolution camera preview and takes a high-resolution picture of the document, recognizing its corners and crops it accordingly, and it dewarps the cropped document to obtain a bird's eye view, sharpens the contrast between the text and the background and finally recognizes the text with formatting kept intact, being further corrected by MonReader's ML powered redactor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Description:**\n",
    "\n",
    "We collected page flipping video from smart phones and labelled them as flipping and not flipping.\n",
    "\n",
    "We clipped the videos as short videos and labelled them as flipping or not flipping. The extracted frames are then saved to disk in a sequential order with the following naming structure: VideoID_FrameNumber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal(s):**\n",
    "\n",
    "Predict if the page is being flipped using a single image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Success Metrics:**\n",
    "\n",
    "Evaluate model performance based on F1 score, the higher the better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus(es):**\n",
    "\n",
    "Predict if a given sequence of images contains an action of flipping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install necessaries libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-09 11:25:41.029458: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/amath/.local/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-05-09 11:25:41.029478: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action = 'ignore')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "import skvideo.io\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from tensorflow import keras\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import (Flatten, Dense, Activation, MaxPooling2D, Conv2D, InputLayer)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "from numba import double, jit, njit, vectorize\n",
    "\n",
    "import progressbar\n",
    "\n",
    "import time\n",
    "\n",
    "import PIL\n",
    "from PIL import Image, ImageEnhance\n",
    "\n",
    "\n",
    "from sklearn.metrics import (confusion_matrix, classification_report, ConfusionMatrixDisplay)\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import pickle\n",
    "\n",
    "import math   # for mathematical operations\n",
    "\n",
    "import pandas as pd\n",
    "from keras.preprocessing import image   # for preprocessing the images\n",
    "import numpy as np    # for mathematical operations\n",
    "from keras.utils import np_utils\n",
    "from skimage.transform import resize   # for resizing images\n",
    "\n",
    "from moviepy.editor import VideoFileClip, concatenate_videoclips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Wrangling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish the path to read all of the images \n",
    "\n",
    "path_training_flip = glob.glob('images/training/flip/*.jpg')\n",
    "\n",
    "path_training_notflip = glob.glob('images/training/notflip/*.jpg')\n",
    "\n",
    "path_testing_flip = glob.glob('images/testing/flip/*.jpg')\n",
    "\n",
    "path_testing_notflip = glob.glob('images/testing/notflip/*.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that will do all the preprocessing for each image to be ready for modeling\n",
    "\n",
    "def image_preprocessing(path):\n",
    "    # Create an empty list to store all the preprocessed images\n",
    "    images = []\n",
    "    # Start by creating a for loop through all the path and make the preprocessing to each image\n",
    "    for i in path:\n",
    "        # Firstly read the image\n",
    "        img = cv2.imread(i)\n",
    "        # Adjust the size so all iamges will have the same size\n",
    "        img = cv2.resize(img, dsize = (70,140), interpolation=cv2.INTER_CUBIC)\n",
    "        # Crop to remove part of the images I don't need for the modeling part\n",
    "        y,h,x,w = 0,100,0,70\n",
    "        img = img[y:y+h, x:x+w]\n",
    "        # Adjust brightness, contrast\n",
    "        alpha=1.5\n",
    "        beta=0.5\n",
    "        img = cv2.addWeighted(img, alpha, np.zeros(img.shape, img.dtype), 0, beta)\n",
    "        # Normalize the images to be black and white by reverting the images and then dividing by 255.0\n",
    "        img = cv2.bitwise_not(img)\n",
    "        img = img/255\n",
    "\n",
    "        # Append the img to the list images\n",
    "        images.append(img)\n",
    "        # Create the video\n",
    "\n",
    "    # Return the list with the preprocessed images\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and preprocess the training data\n",
    "\n",
    "img_training_flip = image_preprocessing(path = path_training_flip)\n",
    "\n",
    "# Read the training not flip\n",
    "\n",
    "img_training_notflip = image_preprocessing(path = path_training_notflip)\n",
    "\n",
    "# Read the test flip\n",
    "\n",
    "img_testing_flip = image_preprocessing(path = path_testing_flip)\n",
    "\n",
    "# Read the test not flip\n",
    "\n",
    "img_testing_notflip = image_preprocessing(path = path_testing_notflip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the labels for the problem\n",
    "\n",
    "y_train_flip = [1 for i in range(0, len(img_training_flip))]\n",
    "\n",
    "y_train_notflip = [0 for i in range(0, len(img_training_notflip))]\n",
    "\n",
    "y_test_flip = [1 for i in range(0, len(img_testing_flip))]\n",
    "\n",
    "y_test_notflip = [0 for i in range(0, len(img_testing_notflip))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the X_train, X_test, y_train and y_test for analysis\n",
    "\n",
    "X_train = np.concatenate((img_training_flip, img_training_notflip), axis = 0)\n",
    "\n",
    "X_test = np.concatenate((img_testing_flip, img_testing_notflip), axis = 0)\n",
    "\n",
    "y_train = np.append(y_train_flip, y_train_notflip)\n",
    "\n",
    "y_test = np.append(y_test_flip, y_test_notflip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2392, 100, 70, 3)\n",
      "(2392,)\n",
      "(597, 100, 70, 3)\n",
      "(597,)\n"
     ]
    }
   ],
   "source": [
    "# See if the shapes matches between the X_trian and y_train and the X_test and y_test\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new array that will have the original arrays (labels and values) but they will be shuffled. \n",
    "\n",
    "# Create the array for the train data set\n",
    "\n",
    "X_train_shuffle = []\n",
    "\n",
    "# It is necessary to create a for loop with enumeration as well\n",
    "for i,j in enumerate(X_train):\n",
    "    # The new array would be the array containing the image plus its label \n",
    "    new_array = (j, y_train[i])\n",
    "    # Append the values to the array that will be shuffled\n",
    "    X_train_shuffle.append(new_array)\n",
    "    \n",
    "# Have the new set of arrays\n",
    "X_train_shuffle = np.array(X_train_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the array for the test data set\n",
    "\n",
    "X_test_shuffle = []\n",
    "\n",
    "# It is necessary to create a for loop with enumeration as well\n",
    "for i,j in enumerate(X_test):\n",
    "    # The new array would be the array containing the image plus its label \n",
    "    new_array = (j, y_test[i])\n",
    "    # Append the values to the array that will be shuffled\n",
    "    X_test_shuffle.append(new_array)\n",
    "    \n",
    "# Have the new set of arrays  \n",
    "X_test_shuffle = np.array(X_test_shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the random shuffle to make the train and test with no specific order\n",
    "\n",
    "np.random.shuffle(X_train_shuffle)\n",
    "\n",
    "np.random.shuffle(X_test_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Separate between the X_train and y_train to fit the model\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "# Start a for loop into the X_train_shuffle\n",
    "for i in X_train_shuffle:\n",
    "    # The array containing the picture would be the one that is in the index 0\n",
    "    value = i[0]\n",
    "    # The label would be the array that is on the index 1\n",
    "    label = i[1]\n",
    "    # Append the values and the labels to separate arrays\n",
    "    X_train.append(value)\n",
    "    y_train.append(label)\n",
    "\n",
    "# Divide between X_train and y_train to run model\n",
    "X_train = np.array(X_train)\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Same for the test data set\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "# Start a for loop into the X_test_shuffle\n",
    "for i in X_test_shuffle:\n",
    "    # The array containing the picture would be the one that is in the index 0\n",
    "    value = i[0]\n",
    "    # The label would be the array that is on the index 1\n",
    "    label = i[1]\n",
    "    # Append the values and the labels to separate arrays\n",
    "    X_test.append(value)\n",
    "    y_test.append(label)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "y_test = np.array(y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2392, 100, 70, 3)\n",
      "(2392,)\n",
      "(597, 100, 70, 3)\n",
      "(597,)\n"
     ]
    }
   ],
   "source": [
    "# Make sure labels are same than the first shapes\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function with the neural networks\n",
    "\n",
    "def neural_network():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation = 'relu', kernel_initializer='he_uniform', \n",
    "                     padding = 'same', input_shape=(100, 70, 3)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile model\n",
    "    opt = SGD(lr=0.001, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-09 11:31:55.317645: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/amath/.local/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-05-09 11:31:55.317957: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-09 11:31:55.318722: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (amath-Latitude-7490): /proc/driver/nvidia/version does not exist\n",
      "2022-05-09 11:31:55.326325: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-09 11:31:55.418263: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 28672000 exceeds 10% of free system memory.\n",
      "2022-05-09 11:31:55.435571: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 28672000 exceeds 10% of free system memory.\n",
      "2022-05-09 11:31:55.457738: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 28672000 exceeds 10% of free system memory.\n",
      "2022-05-09 11:31:56.276367: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 200928000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-09 11:31:58.945278: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 28672000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 22s 286ms/step - loss: 0.4025 - accuracy: 0.8257\n",
      "Epoch 2/15\n",
      "75/75 [==============================] - 5s 65ms/step - loss: 0.1609 - accuracy: 0.9594\n",
      "Epoch 3/15\n",
      "75/75 [==============================] - 4s 56ms/step - loss: 0.0957 - accuracy: 0.9816\n",
      "Epoch 4/15\n",
      "75/75 [==============================] - 4s 56ms/step - loss: 0.0692 - accuracy: 0.9862\n",
      "Epoch 5/15\n",
      "75/75 [==============================] - 5s 70ms/step - loss: 0.0516 - accuracy: 0.9937\n",
      "Epoch 6/15\n",
      "75/75 [==============================] - 5s 61ms/step - loss: 0.0384 - accuracy: 0.9950\n",
      "Epoch 7/15\n",
      "75/75 [==============================] - 4s 56ms/step - loss: 0.0309 - accuracy: 0.9975\n",
      "Epoch 8/15\n",
      "75/75 [==============================] - 4s 56ms/step - loss: 0.0257 - accuracy: 0.9983\n",
      "Epoch 9/15\n",
      "75/75 [==============================] - 4s 55ms/step - loss: 0.0216 - accuracy: 0.9992\n",
      "Epoch 10/15\n",
      "75/75 [==============================] - 4s 54ms/step - loss: 0.0187 - accuracy: 0.9987\n",
      "Epoch 11/15\n",
      "75/75 [==============================] - 4s 58ms/step - loss: 0.0158 - accuracy: 0.9996\n",
      "Epoch 12/15\n",
      "75/75 [==============================] - 4s 57ms/step - loss: 0.0141 - accuracy: 0.9992\n",
      "Epoch 13/15\n",
      "75/75 [==============================] - 4s 55ms/step - loss: 0.0127 - accuracy: 0.9996\n",
      "Epoch 14/15\n",
      "75/75 [==============================] - 4s 53ms/step - loss: 0.0118 - accuracy: 0.9996\n",
      "Epoch 15/15\n",
      "75/75 [==============================] - 5s 60ms/step - loss: 0.0103 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7fa6f2ea30>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model\n",
    "\n",
    "model = neural_network()\n",
    "\n",
    "# fit model\n",
    "model.fit(X_train, y_train, epochs = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Get them into 0 and 1 values\n",
    "\n",
    "binary_values = []\n",
    "\n",
    "# Start a for loop to iterate over the predictions array\n",
    "\n",
    "for i in predictions:\n",
    "    if i < 0.5:\n",
    "        binary_values.append(0)\n",
    "    if i >= 0.5:\n",
    "        binary_values.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the confusion matrix to evaluate the model\n",
    "\n",
    "cm = confusion_matrix(y_test, binary_values)\n",
    "cmd = ConfusionMatrixDisplay(cm, display_labels=['not_flip','flip'])\n",
    "cmd.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       307\n",
      "           1       1.00      0.99      0.99       290\n",
      "\n",
      "    accuracy                           0.99       597\n",
      "   macro avg       0.99      0.99      0.99       597\n",
      "weighted avg       0.99      0.99      0.99       597\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the Classification report to get the precision, recall, f1-score\n",
    "\n",
    "print(classification_report(y_test, binary_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was able to create a model with 0.99 accuray if a page needs to whether be flipped or not by using deep learning and doing the necessary data preprocessing such as making all the pages the same size, adjusting bright, adding nose, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-09 11:34:32.579872: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: flip_page_classifier/assets\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save the model using pickle\n",
    "\n",
    "model_classifier = model.save('flip_page_classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5b3ded1ccb95c1d9bd405e7b823d9e85424cde40fbb5985eb47e999ef50e15b4"
  },
  "kernelspec": {
   "display_name": "",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
